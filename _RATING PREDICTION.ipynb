{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATING PREDICTION PROJECT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted by **GAIRIK** Internship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"image\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM STATEMENT:\n",
    "\n",
    "**We have a client who has a website where people write different\n",
    "reviews for technical products. Now they are adding a new feature to\n",
    "their website i.e. The reviewer will have to add stars(rating) as well\n",
    "with the review. The rating is out 5 stars and it only has 5 options\n",
    "available 1 star, 2 stars, 3 stars, 4 stars, 5 stars. Now they want to\n",
    "predict ratings for the reviews which were written in the past and they\n",
    "don’t have a rating. So, we have to build an application which can\n",
    "predict the rating by seeing the review.**\n",
    "\n",
    "### 1)Data Collection Phase:\n",
    "\n",
    "**You have to scrape at least 20000 rows of data. You can scrape more\n",
    "data as well, it’s up to you. more the data better the model In this\n",
    "section you need to scrape the reviews of different laptops, Phones,\n",
    "Headphones, smart watches, Professional Cameras, Printers, Monitors,\n",
    "Home theater, Router from different e\u0002commerce websites. Basically, we\n",
    "need these columns**\n",
    "\n",
    "-   **1) reviews of the product.**\n",
    "-   **2) rating of the product.**\n",
    "\n",
    "**You can fetch other data as well, if you think data can be useful or\n",
    "can help in the project.It completely depends on your imagination or\n",
    "assumption.**\n",
    "\n",
    "### 2)Model Building Phase:\n",
    "\n",
    "**After collecting the data, you need to build a machine learning model.\n",
    "Before model building do all data preprocessing steps involving NLP. Try\n",
    "different models with different hyper parameters and select the best\n",
    "model.** **Follow the complete life cycle of data science. Include all\n",
    "the steps like**\n",
    "\n",
    "-   **1. Data Cleaning**\n",
    "-   **2. Exploratory Data Analysis**\n",
    "-   **3. Data Preprocessing**\n",
    "-   **4. Model Building**\n",
    "-   **5. Model Evaluation**\n",
    "-   **6. Selecting the best model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Scrapped Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 2,
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have imported the collected data which was in csv format and stored it\n",
    "as a dataframe. We can see the first 5 and last 5 observations of the\n",
    "dataset and it looks good also we have all string valued columns. In\n",
    "this perticular dataset we have about 114491 rows and 3 columns Unnamed:\n",
    "0 is the index column of csv file so let's drop that column. Since\n",
    "**Ratings** is my target column and it is a categorical column with 5\n",
    "categories so this problem is a **Multi Classification Problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Information:\n",
    "\n",
    "-   Review_Title : Title of the Review.\n",
    "-   Review_Text : Text Content of the Review.\n",
    "-   Ratings : Ratings out of 5 stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis \\[EDA\\]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Unnamed: 0 is the index column of csv file so let's drop that\n",
    "column as it will not help us in our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There are 114491 Rows and 3 Columns in the dataset"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 114491 rows and 3 columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Columns present in the dataset are:\n",
      " Index(['Review_Title', 'Review_Text', 'Ratings'], dtype='object')"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above 3 are the column names in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 114491 entries, 0 to 114490\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   Review_Title  98879 non-null   object\n",
      " 1   Review_Text   100658 non-null  object\n",
      " 2   Ratings       98880 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.6+ MB"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the info we can say that there are some null values in the\n",
    "dataset and all the columns are of object data type which means all the\n",
    "entries are string entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Null values in the dataset: \n",
      " Review_Title    15612\n",
      "Review_Text     13833\n",
      "Ratings         15611\n",
      "dtype: int64"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a huge number of nan values in the dataset. Let's replace\n",
    "them using imputation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clear have a look on null values by using visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing nan values using imputation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 9,
     "metadata": {},
     "data": {
      "text/plain": [
       "0    Wonderful\n",
       "dtype: object"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the highest occuring Review_Title is Wonderful, we have to replace\n",
    "the nan values in Review_Title column with it's mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 10,
     "metadata": {},
     "data": {
      "text/plain": [
       "0    Good\n",
       "dtype: object"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the highest occuring Review_Text is Good, we have to replace the nan\n",
    "values in Review_Text column with it's mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 11,
     "metadata": {},
     "data": {
      "text/plain": [
       "5                     19085\n",
       "5.0 out of 5 stars    16301\n",
       "1.0 out of 5 stars    14598\n",
       "4.0 out of 5 stars    14261\n",
       "3.0 out of 5 stars    12313\n",
       "2.0 out of 5 stars    10726\n",
       "4                      6470\n",
       "1                      2249\n",
       "3                      2098\n",
       "2                       779\n",
       "Name: Ratings, dtype: int64"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the highes count in Ratings column is for 5 followed by 5.0 out of\n",
    "5 starts and they both are same so it is clear the mode for Ratings\n",
    "column is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 12,
     "metadata": {},
     "data": {
      "text/plain": [
       "0    5\n",
       "dtype: object"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Null values in the dataset: \n",
      " Review_Title    0\n",
      "Review_Text     0\n",
      "Ratings         0\n",
      "dtype: int64"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now successfully we have replaced all the nan values using imputation\n",
    "method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look into target column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 15,
     "metadata": {},
     "data": {
      "text/plain": [
       "array(['2.0 out of 5 stars', '3.0 out of 5 stars', '1.0 out of 5 stars',\n",
       "       '5.0 out of 5 stars', '5', '4.0 out of 5 stars', '4', '3', '1',\n",
       "       '2'], dtype=object)"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking the above entries in target column we came to know that we need\n",
    "to replace the string entries to there respective values(stars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 17,
     "metadata": {},
     "data": {
      "text/plain": [
       "array([2, 3, 1, 5, 4])"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the target column looks good for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's combine Review_Title and Review_Text to make a single column Review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 19,
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have obtained Review from Review_Title and Review_Text let's\n",
    "drop Review_Title and Review_Text. If not they'll create\n",
    "multicolinearity issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look into our Review column and see first 2 entries how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 21,
     "metadata": {},
     "data": {
      "text/plain": [
       "\"Think twice before going for it \\n  &nbsp;Pros<br>-----<br>- Very light weight<br>- Screen brightness and clarity is awesome<br><br>Cons<br>------<br>- Speaker quality is horrible. Cannot use for calls at all. Crackling noise takes you to vinyl days.<br>- Body is not sturdy. The piece I received was slightly bent at the base, making the laptop wobble when I type.<br>- The bottom screw was hanging out with the thread gone due to incorrect tightening (so much for Japanese craftmanship)<br><br>Recommend to not purchase. It's not worth at this price point.\\n\""
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 22,
     "metadata": {},
     "data": {
      "text/plain": [
       "'Overpriced \\n  Overpriced for this mediocre product with no brand value, better to check out other products in reputed brands like dell, Hp, Asus\\n'"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the Reviews we can say that there are many words, numbers,\n",
    "as well as punctuations which are not important for our predictions. So\n",
    "we need to do good text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look into our text again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 25,
     "metadata": {},
     "data": {
      "text/plain": [
       "'think twice before going for it    nbsppros   very light weight  screen brightness and clarity is awesome  cons   speaker quality is horrible cannot use for calls at all crackling noise takes you to vinyl days  body is not sturdy the piece i received was slightly bent at the base making the laptop wobble when i type  the bottom screw was hanging out with the thread gone due to incorrect tightening so much for japanese craftmanship  recommend to not purchase it is not worth at this price point '"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 26,
     "metadata": {},
     "data": {
      "text/plain": [
       "'overpriced    overpriced for this mediocre product with no brand value better to check out other products in reputed brands like dell hp asus '"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data looks far better than previous.And we have successfully\n",
    "removed punctuations and unwanted text from our text and lowercased all\n",
    "the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing StopWords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 28,
     "metadata": {},
     "data": {
      "text/plain": [
       "'think twice going nbsppros light weight screen brightness clarity awesome cons speaker quality horrible cannot use calls crackling noise takes vinyl days body sturdy piece received slightly bent base making laptop wobble type bottom screw hanging thread gone due incorrect tightening much japanese craftmanship recommend purchase worth price point'"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 29,
     "metadata": {},
     "data": {
      "text/plain": [
       "'overpriced overpriced mediocre product brand value better check products reputed brands like dell hp asus'"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have removed all stop words from the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 34,
     "metadata": {},
     "data": {
      "text/plain": [
       "'think twice go nbsppros light weight screen brightness clarity awesome con speaker quality horrible can not use call crackle noise take vinyl day body sturdy piece receive slightly bent base make laptop wobble type bottom screw hang thread go due incorrect tighten much japanese craftmanship recommend purchase worth price point'"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 35,
     "metadata": {},
     "data": {
      "text/plain": [
       "'overprice overpriced mediocre product brand value well check product repute brand like dell hp asus'"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have removed the inflectional endings and left out with the\n",
    "base or dictionary form of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization - Standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 38,
     "metadata": {},
     "data": {
      "text/plain": [
       "'think twice go nbsppros light weight screen brightness clarity awesome con speaker quality horrible can not use call crackle noise take vinyl day body sturdy piece receive slightly bent base make laptop wobble type bottom screw hang thread go due incorrect tighten much japanese craftmanship recommend purchase worth price point'"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 39,
     "metadata": {},
     "data": {
      "text/plain": [
       "'overprice overpriced mediocre product brand value well check product repute brand like dell hp asus'"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I have defined a function scrub_words for removing the noise\n",
    "from the text. It will remove any html markups, digits and white spaces\n",
    "from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now We did all the text-processing steps and got required input for our\n",
    "model. We will get into Visualization part now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i) Word Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 40,
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the histogram we can clearly see that most of our text is\n",
    "having the number of words in the range of 0 to 200, But some of the\n",
    "reviews are too lengthy which may act like outliers in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii) Character count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 42,
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above plot represents histogram for character count of Review text,\n",
    "which is quite similar to the histogram of word count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Outliers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know that some of the review are too lengthy, so i have to treat\n",
    "them as outliers and remove them using z_score method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 44,
     "metadata": {},
     "data": {
      "text/plain": [
       "(114491, 4)"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 45,
     "metadata": {},
     "data": {
      "text/plain": [
       "(112971, 4)"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great by removing the outliers we are loosing 1.3% of data which is very\n",
    "less and it is in acceptable range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting histograms for word count and character counts again after removing outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting histograms for word counts and character counts and after\n",
    "removing outliers we can see we are left out with good range of number\n",
    "of words and characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iii) Top 30 most frequently occuring words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By seeing the above plot we can see that Good, prodout, quality......are\n",
    "occurring frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iv) Top 30 Rare words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above list of words are have rare occurance in Review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v) Word cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "-   From the above plots we can clearly see the words which are\n",
    "    indication of Reviewer's opinion on products.\n",
    "-   Here most frequent words used for each Rating is displayed in the\n",
    "    word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 58,
     "metadata": {},
     "data": {
      "text/plain": [
       "5    50243\n",
       "4    20378\n",
       "1    16687\n",
       "3    14258\n",
       "2    11405\n",
       "Name: Ratings, dtype: int64"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text data into vectors using Tfidf Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Balancing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train (84728, 150000)\n",
      "x_test (28243, 150000)\n",
      "y_train (84728,)\n",
      "y_test (28243,)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do oversmapling in order to make data balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 65,
     "metadata": {},
     "data": {
      "text/plain": [
       "5    50243\n",
       "4    20378\n",
       "1    16687\n",
       "3    14258\n",
       "2    11405\n",
       "Name: Ratings, dtype: int64"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of classes before fitCounter({5: 37633, 4: 15304, 1: 12568, 3: 10687, 2: 8536})"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have maximum count 37633 for 5ratings hence will over sample\n",
    "mannually all the ratings to the mark 37633."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of classes before fitCounter({5: 37633, 4: 15304, 1: 12568, 3: 10687, 2: 8536})\n",
      "The number of classes after fit Counter({3: 37633, 2: 37633, 1: 37633, 4: 37633, 5: 37633})"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have successfully balanced the data. Let's proceed with model\n",
    "building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "******************************LogisticRegression******************************\n",
      "Accuracy Score: 73.07297383422441\n",
      "---------------------------------------------------\n",
      "CLASSIFICATION REPORT : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.77      0.73      4119\n",
      "           2       0.51      0.49      0.50      2869\n",
      "           3       0.54      0.54      0.54      3571\n",
      "           4       0.60      0.68      0.64      5074\n",
      "           5       0.92      0.85      0.88     12610\n",
      "\n",
      "    accuracy                           0.73     28243\n",
      "   macro avg       0.65      0.67      0.66     28243\n",
      "weighted avg       0.74      0.73      0.73     28243\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 3164   538   284    87    46]\n",
      " [  702  1403   511   208    45]\n",
      " [  385   493  1939   629   125]\n",
      " [  132   207   565  3438   732]\n",
      " [  134   127   285  1370 10694]]\n",
      "\n",
      "******************************LinearSVC******************************\n",
      "Accuracy Score: 76.30209255390716\n",
      "---------------------------------------------------\n",
      "CLASSIFICATION REPORT : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.77      0.75      4119\n",
      "           2       0.56      0.55      0.56      2869\n",
      "           3       0.60      0.59      0.59      3571\n",
      "           4       0.67      0.69      0.68      5074\n",
      "           5       0.91      0.88      0.90     12610\n",
      "\n",
      "    accuracy                           0.76     28243\n",
      "   macro avg       0.69      0.70      0.70     28243\n",
      "weighted avg       0.76      0.76      0.76     28243\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 3184   503   275    94    63]\n",
      " [  615  1587   409   168    90]\n",
      " [  329   454  2100   496   192]\n",
      " [  109   173   466  3520   806]\n",
      " [   96   113   273   969 11159]]\n",
      "\n",
      "******************************DecisionTreeClassifier******************************\n",
      "Accuracy Score: 72.26569415430372\n",
      "---------------------------------------------------\n",
      "CLASSIFICATION REPORT : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.68      0.69      4119\n",
      "           2       0.50      0.55      0.52      2869\n",
      "           3       0.56      0.55      0.56      3571\n",
      "           4       0.63      0.65      0.64      5074\n",
      "           5       0.88      0.85      0.86     12610\n",
      "\n",
      "    accuracy                           0.72     28243\n",
      "   macro avg       0.65      0.66      0.65     28243\n",
      "weighted avg       0.73      0.72      0.72     28243\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 2810   567   327   231   184]\n",
      " [  516  1565   352   262   174]\n",
      " [  358   433  1971   467   342]\n",
      " [  190   297   473  3318   796]\n",
      " [  190   242   406  1026 10746]]\n",
      "\n",
      "******************************SGDClassifier******************************\n",
      "Accuracy Score: 72.75077010232624\n",
      "---------------------------------------------------\n",
      "CLASSIFICATION REPORT : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.85      0.72      4119\n",
      "           2       0.51      0.41      0.45      2869\n",
      "           3       0.61      0.44      0.51      3571\n",
      "           4       0.62      0.63      0.62      5074\n",
      "           5       0.88      0.88      0.88     12610\n",
      "\n",
      "    accuracy                           0.73     28243\n",
      "   macro avg       0.65      0.64      0.64     28243\n",
      "weighted avg       0.72      0.73      0.72     28243\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 3512   299   136    94    78]\n",
      " [ 1037  1169   341   226    96]\n",
      " [  600   528  1559   617   267]\n",
      " [  236   200   341  3177  1120]\n",
      " [  191    94   170  1025 11130]]\n",
      "\n",
      "******************************RandomForestClassifier******************************\n",
      "Accuracy Score: 77.58736678114931\n",
      "---------------------------------------------------\n",
      "CLASSIFICATION REPORT : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.84      0.76      4119\n",
      "           2       0.65      0.53      0.58      2869\n",
      "           3       0.68      0.56      0.62      3571\n",
      "           4       0.66      0.73      0.69      5074\n",
      "           5       0.91      0.89      0.90     12610\n",
      "\n",
      "    accuracy                           0.78     28243\n",
      "   macro avg       0.72      0.71      0.71     28243\n",
      "weighted avg       0.78      0.78      0.77     28243\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 3469   306   154   128    62]\n",
      " [  744  1515   315   216    79]\n",
      " [  451   340  1997   565   218]\n",
      " [  159   116   293  3714   792]\n",
      " [  165    70   158   999 11218]]\n",
      "\n",
      "******************************XGBClassifier******************************\n",
      "Accuracy Score: 73.05881103282229\n",
      "---------------------------------------------------\n",
      "CLASSIFICATION REPORT : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.76      0.73      4119\n",
      "           2       0.46      0.50      0.48      2869\n",
      "           3       0.56      0.49      0.52      3571\n",
      "           4       0.62      0.66      0.64      5074\n",
      "           5       0.91      0.87      0.89     12610\n",
      "\n",
      "    accuracy                           0.73     28243\n",
      "   macro avg       0.65      0.66      0.65     28243\n",
      "weighted avg       0.74      0.73      0.73     28243\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 3142   616   210    97    54]\n",
      " [  718  1445   428   224    54]\n",
      " [  393   658  1747   602   171]\n",
      " [  133   242   497  3374   828]\n",
      " [  107   165   249  1163 10926]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have created 6 different classification algorithms. Great, among all\n",
    "these algorithms all are giving good accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "******************************LogisticRegression******************************\n",
      "Cross validation score : 58.2556585318356\n",
      "******************************LinearSVC******************************\n",
      "Cross validation score : 57.08544670756211\n",
      "******************************DecisionTreeClassifier******************************\n",
      "Cross validation score : 50.68291862513389\n",
      "******************************SGDClassifier******************************\n",
      "Cross validation score : 58.57343920121093\n",
      "******************************RandomForestClassifier******************************\n",
      "Cross validation score : 58.619468713209585\n",
      "******************************XGBClassifier******************************\n",
      "Cross validation score : 58.42295810429224"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great all our algorithms are giving good cv scores.Among these\n",
    "algorithms I am selecting SGD Classifier as best fitting algorithm for\n",
    "our final model as it is giving least difference between accuracy and cv\n",
    "score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV 1/3; 1/12] START loss=hinge, n_jobs=-1, penalty=l2..........................\n",
      "[CV 1/3; 1/12] END loss=hinge, n_jobs=-1, penalty=l2;, score=0.708 total time= 2.2min\n",
      "[CV 2/3; 1/12] START loss=hinge, n_jobs=-1, penalty=l2..........................\n",
      "[CV 2/3; 1/12] END loss=hinge, n_jobs=-1, penalty=l2;, score=0.787 total time=   4.4s\n",
      "[CV 3/3; 1/12] START loss=hinge, n_jobs=-1, penalty=l2..........................\n",
      "[CV 3/3; 1/12] END loss=hinge, n_jobs=-1, penalty=l2;, score=0.791 total time=   4.4s\n",
      "[CV 1/3; 2/12] START loss=hinge, n_jobs=-1, penalty=l1..........................\n",
      "[CV 1/3; 2/12] END loss=hinge, n_jobs=-1, penalty=l1;, score=0.524 total time=   7.2s\n",
      "[CV 2/3; 2/12] START loss=hinge, n_jobs=-1, penalty=l1..........................\n",
      "[CV 2/3; 2/12] END loss=hinge, n_jobs=-1, penalty=l1;, score=0.580 total time=   7.5s\n",
      "[CV 3/3; 2/12] START loss=hinge, n_jobs=-1, penalty=l1..........................\n",
      "[CV 3/3; 2/12] END loss=hinge, n_jobs=-1, penalty=l1;, score=0.584 total time=   7.3s\n",
      "[CV 1/3; 3/12] START loss=hinge, n_jobs=-1, penalty=elasticnet..................\n",
      "[CV 1/3; 3/12] END loss=hinge, n_jobs=-1, penalty=elasticnet;, score=0.597 total time=  11.0s\n",
      "[CV 2/3; 3/12] START loss=hinge, n_jobs=-1, penalty=elasticnet..................\n",
      "[CV 2/3; 3/12] END loss=hinge, n_jobs=-1, penalty=elasticnet;, score=0.666 total time=  10.9s\n",
      "[CV 3/3; 3/12] START loss=hinge, n_jobs=-1, penalty=elasticnet..................\n",
      "[CV 3/3; 3/12] END loss=hinge, n_jobs=-1, penalty=elasticnet;, score=0.671 total time=   9.9s\n",
      "[CV 1/3; 4/12] START loss=hinge, n_jobs=1, penalty=l2...........................\n",
      "[CV 1/3; 4/12] END loss=hinge, n_jobs=1, penalty=l2;, score=0.708 total time=   8.0s\n",
      "[CV 2/3; 4/12] START loss=hinge, n_jobs=1, penalty=l2...........................\n",
      "[CV 2/3; 4/12] END loss=hinge, n_jobs=1, penalty=l2;, score=0.789 total time=   8.5s\n",
      "[CV 3/3; 4/12] START loss=hinge, n_jobs=1, penalty=l2...........................\n",
      "[CV 3/3; 4/12] END loss=hinge, n_jobs=1, penalty=l2;, score=0.792 total time=   8.4s\n",
      "[CV 1/3; 5/12] START loss=hinge, n_jobs=1, penalty=l1...........................\n",
      "[CV 1/3; 5/12] END loss=hinge, n_jobs=1, penalty=l1;, score=0.538 total time=  12.4s\n",
      "[CV 2/3; 5/12] START loss=hinge, n_jobs=1, penalty=l1...........................\n",
      "[CV 2/3; 5/12] END loss=hinge, n_jobs=1, penalty=l1;, score=0.567 total time=  12.2s\n",
      "[CV 3/3; 5/12] START loss=hinge, n_jobs=1, penalty=l1...........................\n",
      "[CV 3/3; 5/12] END loss=hinge, n_jobs=1, penalty=l1;, score=0.572 total time=  12.2s\n",
      "[CV 1/3; 6/12] START loss=hinge, n_jobs=1, penalty=elasticnet...................\n",
      "[CV 1/3; 6/12] END loss=hinge, n_jobs=1, penalty=elasticnet;, score=0.599 total time=  21.0s\n",
      "[CV 2/3; 6/12] START loss=hinge, n_jobs=1, penalty=elasticnet...................\n",
      "[CV 2/3; 6/12] END loss=hinge, n_jobs=1, penalty=elasticnet;, score=0.676 total time=  20.3s\n",
      "[CV 3/3; 6/12] START loss=hinge, n_jobs=1, penalty=elasticnet...................\n",
      "[CV 3/3; 6/12] END loss=hinge, n_jobs=1, penalty=elasticnet;, score=0.672 total time=  22.4s\n",
      "[CV 1/3; 7/12] START loss=squared_hinge, n_jobs=-1, penalty=l2..................\n",
      "[CV 1/3; 7/12] END loss=squared_hinge, n_jobs=-1, penalty=l2;, score=0.570 total time= 6.4min\n",
      "[CV 2/3; 7/12] START loss=squared_hinge, n_jobs=-1, penalty=l2..................\n",
      "[CV 2/3; 7/12] END loss=squared_hinge, n_jobs=-1, penalty=l2;, score=0.775 total time= 6.5min\n",
      "[CV 3/3; 7/12] START loss=squared_hinge, n_jobs=-1, penalty=l2..................\n",
      "[CV 3/3; 7/12] END loss=squared_hinge, n_jobs=-1, penalty=l2;, score=0.791 total time= 6.9min\n",
      "[CV 1/3; 8/12] START loss=squared_hinge, n_jobs=-1, penalty=l1..................\n",
      "[CV 1/3; 8/12] END loss=squared_hinge, n_jobs=-1, penalty=l1;, score=0.693 total time=29.7min\n",
      "[CV 2/3; 8/12] START loss=squared_hinge, n_jobs=-1, penalty=l1..................\n",
      "[CV 2/3; 8/12] END loss=squared_hinge, n_jobs=-1, penalty=l1;, score=0.804 total time=28.5min\n",
      "[CV 3/3; 8/12] START loss=squared_hinge, n_jobs=-1, penalty=l1..................\n",
      "[CV 3/3; 8/12] END loss=squared_hinge, n_jobs=-1, penalty=l1;, score=0.809 total time=28.4min\n",
      "[CV 1/3; 9/12] START loss=squared_hinge, n_jobs=-1, penalty=elasticnet..........\n",
      "[CV 1/3; 9/12] END loss=squared_hinge, n_jobs=-1, penalty=elasticnet;, score=0.569 total time=27.6min\n",
      "[CV 2/3; 9/12] START loss=squared_hinge, n_jobs=-1, penalty=elasticnet..........\n",
      "[CV 2/3; 9/12] END loss=squared_hinge, n_jobs=-1, penalty=elasticnet;, score=0.780 total time=27.9min\n",
      "[CV 3/3; 9/12] START loss=squared_hinge, n_jobs=-1, penalty=elasticnet..........\n",
      "[CV 3/3; 9/12] END loss=squared_hinge, n_jobs=-1, penalty=elasticnet;, score=0.794 total time=28.4min\n",
      "[CV 1/3; 10/12] START loss=squared_hinge, n_jobs=1, penalty=l2..................\n",
      "[CV 1/3; 10/12] END loss=squared_hinge, n_jobs=1, penalty=l2;, score=0.569 total time=12.0min\n",
      "[CV 2/3; 10/12] START loss=squared_hinge, n_jobs=1, penalty=l2..................\n",
      "[CV 2/3; 10/12] END loss=squared_hinge, n_jobs=1, penalty=l2;, score=0.776 total time=11.9min\n",
      "[CV 3/3; 10/12] START loss=squared_hinge, n_jobs=1, penalty=l2..................\n",
      "[CV 3/3; 10/12] END loss=squared_hinge, n_jobs=1, penalty=l2;, score=0.789 total time=11.9min\n",
      "[CV 1/3; 11/12] START loss=squared_hinge, n_jobs=1, penalty=l1..................\n",
      "[CV 1/3; 11/12] END loss=squared_hinge, n_jobs=1, penalty=l1;, score=0.628 total time=52.0min\n",
      "[CV 2/3; 11/12] START loss=squared_hinge, n_jobs=1, penalty=l1..................\n",
      "[CV 2/3; 11/12] END loss=squared_hinge, n_jobs=1, penalty=l1;, score=0.803 total time=53.1min\n",
      "[CV 3/3; 11/12] START loss=squared_hinge, n_jobs=1, penalty=l1..................\n",
      "[CV 3/3; 11/12] END loss=squared_hinge, n_jobs=1, penalty=l1;, score=0.812 total time=53.5min\n",
      "[CV 1/3; 12/12] START loss=squared_hinge, n_jobs=1, penalty=elasticnet..........\n",
      "[CV 1/3; 12/12] END loss=squared_hinge, n_jobs=1, penalty=elasticnet;, score=0.576 total time=51.2min\n",
      "[CV 2/3; 12/12] START loss=squared_hinge, n_jobs=1, penalty=elasticnet..........\n",
      "[CV 2/3; 12/12] END loss=squared_hinge, n_jobs=1, penalty=elasticnet;, score=0.778 total time=52.9min\n",
      "[CV 3/3; 12/12] START loss=squared_hinge, n_jobs=1, penalty=elasticnet..........\n",
      "[CV 3/3; 12/12] END loss=squared_hinge, n_jobs=1, penalty=elasticnet;, score=0.796 total time=52.0min"
     ]
    },
    {
     "output_type": "execute_result",
     "execution_count": 76,
     "metadata": {},
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=SGDClassifier(),\n",
       "             param_grid={'loss': ['hinge', 'squared_hinge'], 'n_jobs': [-1, 1],\n",
       "                         'penalty': ['l2', 'l1', 'elasticnet']},\n",
       "             verbose=10)"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 77,
     "metadata": {},
     "data": {
      "text/plain": [
       "{'loss': 'squared_hinge', 'n_jobs': -1, 'penalty': 'l1'}"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy Score : 68.71083100237227\n",
      "\n",
      "Confusion Matrix : \n",
      " [[ 2716   651   403   192   157]\n",
      " [  649  1291   490   273   166]\n",
      " [  433   502  1753   553   330]\n",
      " [  223   292   592  2956  1011]\n",
      " [  186   201   430  1103 10690]]\n",
      "\n",
      "\n",
      "CLASSIFICATION REPORT : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.66      0.65      4119\n",
      "           2       0.44      0.45      0.44      2869\n",
      "           3       0.48      0.49      0.48      3571\n",
      "           4       0.58      0.58      0.58      5074\n",
      "           5       0.87      0.85      0.86     12610\n",
      "\n",
      "    accuracy                           0.69     28243\n",
      "   macro avg       0.60      0.61      0.60     28243\n",
      "weighted avg       0.69      0.69      0.69     28243\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter tuning we are unable to improved our model\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Saving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 81,
     "metadata": {},
     "data": {
      "text/plain": [
       "['Ratings_RP.pkl']"
      ]
     }
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I have saved the model into .pkl file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "### Key findings of the study :\n",
    "\n",
    "-   In this project I have collected data of reviews and ratings for\n",
    "    different products from amazon.in and flipkart.com.\n",
    "-   Then I have done different text processing for reviews column and\n",
    "    chose equal number of text from each rating class to eliminate\n",
    "    problem of imbalance.\n",
    "-   By doing different EDA steps I have analyzed the text. We have\n",
    "    checked frequently occurring words in our data as well as rarely\n",
    "    occurring words.\n",
    "-   After all these steps I have built function to train different\n",
    "    algorithms and using various evaluation metrics I have selected\n",
    "    SGDClassifier for our final model.\n",
    "-   Finally by doing hyperparameter tuning we got optimum parameters for\n",
    "    our final model. And finally we got good accuracy score for our\n",
    "    final model.\n",
    "\n",
    "### Limitations of this work and scope for the future work :\n",
    "\n",
    "As we know the content of text in reviews is totally depends on the\n",
    "reviewer and they may rate differently which is totally depends on that\n",
    "particular person. So it is difficult to predict ratings based on the\n",
    "reviews with higher accuracies. Still we can improve our accuracy by\n",
    "fetching more data and by doing extensive hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"image\"></span>"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
